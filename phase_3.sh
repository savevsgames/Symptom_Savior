# PHASE 3: AI and Voice Features
gh issue create --title "Create AI Assistant chat interface screen" --body "Description: Develop the UI for the AI Assistant chat where the user can interact with their Guardian AI. The screen should resemble a messaging interface: a scrollable list of chat bubbles user messages on right AI responses on left perhaps and an input area to type a question or use voice. Implement the basic layout in app/(tabs)/assistant.tsx. Initially show a welcome or instruction e.g. Ask about your symptoms or health concerns. The input area includes a TextInput for typing and a microphone button for voice which will be handled in a later issue. Pressing send a paper plane icon or just the keyboard submit should trigger the query to the AI integration handled next issue. For now focus on structure: create a messages state array that will hold objects like sender: user|ai text. Display these in a FlatList or ScrollView styled as chat bubbles you can create a ChatMessage component. Ensure the view auto-scrolls to the bottom when a new message is added. Also include a loading indicator or placeholder while awaiting a response for instance a typing message or spinner. The design should keep the same theme use colors consistent with brand: perhaps user bubbles in blue AI in gray and use the Inter font. File Paths: app/(tabs)/assistant.tsx new file if not already created. Create a components/ChatMessage.tsx for the bubble component if needed. Use SafeAreaView etc similar to other screens. Component Reuse: Use BaseTextInput for the message input box. The microphone button can use the VoiceInput component from Phase 0 if implemented. Chat bubbles can be a styled View with Text; not necessarily in the base components but make sure to use consistent styling maybe even leverage RNRs Alert or Card components for bubble styling with some tweak. Data Sources: No external data in this issue; well add actual AI calls next. Possibly use dummy data to test like pre-populate one AI greeting. Expo Router Navigation: This is a tab screen already part of tabs. Just ensure the route exists and is linked Dashboard Ask Guardian button pushes to it. The back behavior is just the tab switch no back arrow needed since its a root of a tab. Acceptance Criteria: 1. The Assistant screen UI is implemented with an input at the bottom and a message list. The layout should be tested on different screen sizes including proper keyboard handling e.g. input not covered by keyboard use KeyboardAvoidingView if necessary. 2. Styling is clean and distinct between user and AI messages different bubble colors or alignment. 3. The user can type a message and hitting send will add their message to the list for now maybe immediately also add a placeholder AI response like ... demonstrating the flow. 4. The screen is accessible via the tab navigator and from the Dashboards Chat with Guardian/Ask Guardian buttons. No crashes or UI breakages occur when navigating to this screen." --label "phase:3,frontend,ai,expo"

gh issue create --title "Integrate AI query handling using TxAgent RAG API" --body "Description: Connect the chat interface to the backend AI TxAgent so that user questions get real answers. When the user submits a query text call the Medical Consultation API endpoint provided by the TxAgent container. This likely is an HTTP POST to something like /api/medical-consultation on our backend or directly to TxAgents URL depending on networking setup. Include in the request: the question text and any necessary context. According to the integration design we should send query user question and possibly the users recent symptoms or profile if needed. Also set include_voice: true if we want a voice URL back and include_video: false unless we plan to test Tavus now. The call must include the users Supabase JWT in Authorization header so the container can use it for RLS and context. Implement this API call using fetch or Axios with the endpoint base URL from config. Show a loading state while awaiting response e.g. disable input show Guardian is thinking message. Once a response arrives it will contain a JSON payload with the answer text and possibly sources etc. Take the response.text field from JSON and add it as a new message in the chat from AI. Also if response.sources exists list of docs used you might append a message like Sources: ... or handle it in UI maybe not in MVP unless easy. The API may also return an urgent_care_recommended or disclaimer field ensure to display the disclaimer either as a system message or below the AI answer since its required for safety. If the request fails or times out handle gracefully: show an error message to user e.g. Unable to get response please try again. File Paths: app/(tabs)/assistant.tsx add an async function to call the API. Also possibly create a helper in util/api.ts if needed. Use the supabase.auth.getSession() to retrieve JWT for Authorization or if the supabase client stores it use that. The TxAgent base URL and any keys should come from environment config Phase 0. Component Reuse: Use the logger to log query/response for debugging. The chat UI from previous issue is reused here to append messages. Possibly reuse some loading indicator component for the thinking state. Data Sources: TxAgent RAG API an external service that processes the query using medical documents. It will utilize the users symptom history and profile if configured since it shares the DB and JWT. Ensure the users past symptom logs are accessible they should be via RLS. The request might include a symptoms field; consider including recent symptoms: e.g. take the last few from user_symptoms or allow the backend to fetch them if its designed to. Initially you can omit or include an empty symptoms: [] if not sure the container will still answer general questions. Expo Router Navigation: N/A stays on same screen. However if an answer suggests an emergency we handle that in Phase 6 possibly navigate or prompt something to be done later. Acceptance Criteria: 1. When a user sends a message in the chat the app successfully calls the AI consultation API and receives a response. The AIs answer appears in the chat UI as a new message from Guardian. 2. The content of the answer is coherent and obviously coming from the backend to test ask a question like What could cause a headache and see a plausible medical answer not the dummy placeholder. 3. The app includes the required authorization so that the AI can use user-specific data. For example if the users profile or symptom history is meant to influence the answer the JWT ensures the backend can query those tables with RLS. 4. The chat interface remains responsive: user cannot spam multiple queries while one is in progress you might disable the send button while awaiting reply. Errors are handled with a user-friendly notice and re-enable input so they can retry. 5. The medical disclaimer e.g. This is not a substitute for professional medical advice is displayed with each response either appended to the message or as a separate text in the UI satisfying safety requirements." --label "phase:3,backend,ai,expo"

gh issue create --title "Enable text-to-speech playback of AI responses ElevenLabs integration" --body "Description: Provide the option for users to hear the AIs responses spoken aloud using the ElevenLabs voice API. After receiving the AIs response text in the previous issue utilize the responses voice_audio_url if available. The TxAgent consultation endpoint can be configured to return a pregenerated URL for the TTS audio. Assuming include_voice: true was sent the JSON should contain media.voice_audio_url and possibly a delay if it was still processing. If the voice URL is ready in the response use Expos Audio API or expo-av to play that audio. Implement a small play button icon next to the AI message or automatically play it when it arrives maybe configurable via user preference. If the API does not return a URL the alternative is to call ElevenLabs API directly: sending the text and getting back audio data. However doing that client-side exposes the API key so preferably the backend handles it. For MVP rely on the provided URL. Ensure the app has permission to play sound just needs audio which is fine. Manage the playback state: if an answer is long allow the user to pause/stop? Could be overkill simply play to completion. Also handle if multiple messages come in quickly queue or stop previous. File Paths: app/(tabs)/assistant.tsx after adding the AI message to state initiate audio playback using Audio.Sound. No new UI file but you might adjust ChatMessage component to include a play icon or waveform animation while playing. Also ensure ELEVENLABS_API_KEY is in env if direct API usage was needed likely not if using voice URL. Component Reuse: No specific base component but consider using a small button component for play/pause if needed. Data Sources: The voice_audio_url from the AI response likely pointing to an MP3 in Supabase storage or ElevenLabs CDN. If thats not implemented yet in backend an alternative is calling generateVoiceResponse similar to the reference code after receiving text. For now assume backend does it. Expo Router Navigation: N/A within chat screen. Acceptance Criteria: 1. After the AI responds the user can hear the answer spoken in a realistic voice. If the design is to autoplay it should begin within a second or two of the message appearing. If manual the user taps a play icon. 2. The voice used is consistent e.g. a default medical assistant voice as set in config. No robotic TTS defaults from OS; it should be ElevenLabs quality. 3. There is no significant lag or blocking of the UI while audio is fetched or played. The user can continue to scroll or even send another query though if they do perhaps stop the current playback to avoid overlap. 4. If the voice API fails or no audio available the app should handle it quietly maybe the play option doesnt show or an error is logged but the text answer is still there so the user isnt blocked. 5. This fulfills the voice output feature: users who prefer auditory responses have a working solution." --label "phase:3,frontend,ai,voice"

gh issue create --title "Support voice input speech-to-text for querying the AI" --body "Description: Allow users to speak their questions instead of typing. Implement audio recording and speech-to-text conversion on the Assistant screen. When the user presses the microphone button VoiceInput component start recording audio from the devices microphone. Use Expos Audio.Recording or a community module to capture the audio preferably in a format the backend expects e.g. 16kHz wav. Provide UI feedback: change the mic icon or show Recording while active. On release or after a max duration stop recording and obtain the audio file data. Then send this audio to a speech-to-text API. The design suggests using ElevenLabs for STT as well possibly via a dedicated endpoint like /api/voice/transcribe. So perform a fetch POST to api/voice/transcribe with the audio file as multipart/form-data and the users JWT. The backend should return JSON with a transcription text. Once received treat that text as if the user typed it: display it in the chat from the user and proceed to send it to the AI query handler so it triggers the answer. Essentially this automates the ask. Handle error cases if transcription fails show an error or allow retry. Also consider noise or empty input: if the user doesnt speak or its silent you might handle that maybe the STT returns empty and you prompt user to try again. File Paths: components/VoiceInput.tsx implement the recording logic and state here so that pressing the component in the Assistant screen triggers recording. Alternatively handle in assistant.tsx directly if simpler. Also create util/audio.ts if needed for recording utility. Make sure to request microphone permissions on app startup or when first using the feature. Component Reuse: The VoiceInput button from Phase 0 should visually indicate recording could change icon to a stop button or red circle. If not already created style a TouchableOpacity accordingly. Data Sources: ElevenLabs STT or chosen STT service. The integration likely goes through our backend as per the reference design so we call our /api/voice/transcribe which then uses ElevenLabs or Whisper to transcribe. This keeps the API key secret. Ensure the endpoint is set in config and accessible. Expo Router Navigation: N/A within chat screen. Acceptance Criteria: 1. The user can hold or tap the microphone button speak their question and upon releasing or tapping again the speech is recorded and converted to text. This transcribed text then appears in the chat input or directly as a sent message from the user. 2. The transcription accuracy is good within reason for a medical context. Test by saying a sample query I have had a headache for 2 days what should I do and verify the text is recognized correctly or with minimal errors. 3. The voice recording handles permissions the first time user is asked for microphone access and if denied an appropriate fallback or message is provided. 4. The UI provides feedback while recording so the user knows their voice is being captured. And if the user cancels maybe by pressing again it stops gracefully. 5. End-to-end: After speaking the question gets answered by the AI and the voice answer can play making the entire voice input -> AI -> voice output loop functional." --label "phase:3,frontend,ai,voice"

gh issue create --title "Log AI consultation interactions in database" --body "Description: Record each AI consultation Q&A exchange to the database for history and analysis. Use the medical_consultations table in Supabase to store these. After receiving a response from the AI in the chat handler construct a record with fields: user_id session_id could group a series of Q&A but for MVP each question can be its own session or use a chat session ID if we implement multi-turn context the query text the response text sources JSON of any sources returned voice_audio_url if any video_url if any probably empty now consultation_type e.g. symptom_inquiry processing_time could measure how long the AI took and timestamp. The Supabase RLS will ensure only the user sees their consultations. Implement this by calling supabase.from medical_consultations .insert after receiving the AIs reply. This can be done in the background no need to block user but ensure it happens maybe await it but not strictly needed to show UI. If we have a session_id concept generate one like keep an ongoing chat session ID. Even if each question is separate record it anyway. This data will be useful for analytics and for doctors the prompt mentioned sharing DB with doctors portal. Handle insert errors silently log them to not disturb the user chat flow. File Paths: The logic likely goes in assistant.tsx where the AI response is handled. Possibly create a helper to log consultation could be in util/api.ts. Component Reuse: N/A no UI. Data Sources: Supabase medical_consultations table. Ensure it exists with proper schema and RLS from integration doc. If not coordinate creation similar to user_symptoms. Expo Router Navigation: N/A stays on chat this is backend logging. In future we might have a screen to view past consultations but not in current scope. Acceptance Criteria: 1. Every time the AI provides an answer to the user a new row is added to medical_consultations in the database with the details of that Q&A. Verify by checking the table after some interactions it should show the questions asked and the responses given. 2. The data saved is comprehensive: the full text of the question and answer so that even if the user clears chat or something its recorded and the sources if any e.g. which documents were referenced as well as timestamps. 3. The operation does not noticeably impact the user experience logging happens quickly in the background. Even if it fails no network etc the user still got their answer; the failure is non-critical. 4. This logging aligns with the privacy constraints only the user and authorized staff can see it. RLS is in effect tested by ensuring one user cannot fetch anothers consultations." --label "phase:3,backend,supabase,ai"

# CREATED - JUNE 19 - 2025
# Creating issue in savevsgames/Symptom_Savior
# https://github.com/savevsgames/Symptom_Savior/issues/13
# Creating issue in savevsgames/Symptom_Savior
# https://github.com/savevsgames/Symptom_Savior/issues/14
# Creating issue in savevsgames/Symptom_Savior
# https://github.com/savevsgames/Symptom_Savior/issues/15
# Creating issue in savevsgames/Symptom_Savior
# https://github.com/savevsgames/Symptom_Savior/issues/16
# Creating issue in savevsgames/Symptom_Savior
# https://github.com/savevsgames/Symptom_Savior/issues/17